---
layout: post
title: XLNet论文阅读
date: 2020-08-07 21:11 +0800
tags:
    - nlp
    - article
hero: https://source.unsplash.com/collection/582860/
overlay: blue

---


<!--enable mathjax-->
<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

# XLNet 论文阅读

## 背景

### BERT属于AutoEncoder语言模型
<!–-break-–>
- BERT优点
    - 双向编码，考虑上下文信息
    - 使用self-attention，支持并行
    - 相较于RNN，LSTM，没有梯度累积效应

- BERT缺点
    - 测试数据没有\[MASK\]
    - 本质上属于DAE(denoising auto encoder),缺乏生成能力
    - 在denoising的过程中，并没有利用到不同\[MASK\]之间的相关性
    $P(m_{3},m_{4}|m_{1},m_{2},m_{5})=P(m_{3}|m_{1},m_{2},m_{5})\times P(m_{4}|m_{1},m_{2},m_{5})$
    <br>即各\[MASK\]之间是条件独立的
    
### Autoregressive模型
- 优点
    - 具备生成能力
    - 考虑到了词之间的相关性
    - 可以支持无监督训练
    - 有严格的数学表达
    
- 缺点
    - 单向建模，无法获得上下文信息
    - 离得近，未必有关系
    
能否将两类模型进行融合，从而克服BERT所面临的问题

## 如何做

基于AR语言模型进行改良，从而支持双向encode的能力

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="2020-08-07-xlnet/AR-objective.jpg">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">AR语言模型目标函数</div>
</center>

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="2020-08-07-xlnet/AE-objective-1.jpg">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">AE语言模型目标函数</div>
    <br><br>
</center>

$$
m_{t}=
\left\{\begin{matrix}
1 & \text{if masked}\\
0 & \text{otherwise}\\
\end{matrix}\right.
$$

$$h_{\theta }\left ( \hat{x}\right ):\text{representation of unmasked tokens}$$